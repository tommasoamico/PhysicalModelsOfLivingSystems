{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "earned-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import comet_ml #must be before torch\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "\n",
    "from typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "\n",
    "from helper import ReshapeTransform\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from mpl2latex import mpl2latex, latex_figsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eligible-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load (normalized) MNIST dataset, split into train/validation/test, reshape to 1D vectors.\n",
    "\n",
    "#From: https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_dir : str = os.getcwd(),\n",
    "                 train_batch_size : int = 16,\n",
    "                 test_batch_size  : int = 1000):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #---Store arguments---#\n",
    "        self.data_dir = data_dir\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.test_batch_size  = test_batch_size\n",
    "        \n",
    "        #---Transforms---#\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.double()),\n",
    "        ])\n",
    "        \n",
    "        self.dims = (1,28,28)\n",
    "        \n",
    "    def setup(self, stage : Optional[str] = None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transforms)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [50000, 10000]) #Train/Validation split\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test  = MNIST(self.data_dir, train=False, transform=self.transforms)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.train_batch_size, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.test_batch_size, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.test_batch_size, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "narrative-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Local(Optimizer):\n",
    "    \"\"\"\n",
    "    A simple optimizer that changes weights according to `weights += learning_rate * delta_weights`,\n",
    "    for a provided `delta_weights`, automatically retrieving the `learning_rate` from each layer metadata.\n",
    "    \n",
    "    Taken from: https://github.com/Joxis/pytorch-hebbian/blob/595ec79577c3816c61d39b0633f8dbf14d28b67a/pytorch_hebbian/optimizers/local.py#L15\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, named_params, lr=required):\n",
    "        self.param_names, params = zip(*named_params)\n",
    "        \n",
    "        if lr is not required and lr < 0.0: #?\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "    \n",
    "        defaults = dict(lr=lr)\n",
    "        super(Local, self).__init__(params, defaults)\n",
    "    \n",
    "    def local_step(self, delta_weights, layer_name, closure=None):\n",
    "        \"\"\"Performs a single local optimization step: weights += learning_rate * delta_weights.\n",
    "        The needed learning_rate is automatically retrieved from the layer metadata.\"\"\"\n",
    "        \n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "            \n",
    "        for group in self.param_groups:\n",
    "            layer_index = self.param_names.index(layer_name + '.weight') #find index of layer\n",
    "            weights = group['params'][layer_index]\n",
    "            weights.data.add_(group['lr'] * delta_weights)\n",
    "            #print(\"Add weights to \", layer_index)\n",
    "        \n",
    "        try:\n",
    "            self._step_count += 1\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "british-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioLearningRule():\n",
    "    \"\"\"\n",
    "    Implements the bio-inspired Hebbian learning rule from [1]. \n",
    "    \n",
    "    Adapted from https://github.com/Joxis/pytorch-hebbian/blob/master/pytorch_hebbian/learning_rules/krotov.py\n",
    "    \n",
    "    [1]: \"Unsupervised learning by competing hidden units\", D. Krotov, J. J. Hopfield, 2019, \n",
    "         https://www.pnas.org/content/116/16/7723\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 precision     : float = 1e-30,\n",
    "                 delta         : float = 0.4,\n",
    "                 lebesgue_p    : float = 2.0, \n",
    "                 ranking_param : int   = 2):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        precision : float\n",
    "            Minimum value that is considered non-zero.\n",
    "        delta : float\n",
    "            Strength of anti-Hebbian learning (from eq. 9 in [1]). \n",
    "        lebesgue_p : float\n",
    "            Parameter for Lebesgue measure, used for defining an inner product (from eq. 2 in [1]).\n",
    "        ranking_param: int\n",
    "            Rank of the current to which anti-hebbian learning is applied. Should be >= 2. This is the `k` from eq. 10 in [1].\n",
    "        \"\"\"\n",
    "        \n",
    "        #Store hyperparameters\n",
    "        self.precision     = precision\n",
    "        self.delta         = delta\n",
    "        self.lebesgue_p    = lebesgue_p\n",
    "        self.ranking_param = ranking_param\n",
    "        \n",
    "        print(self)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"Return string representation of the object\"\"\"\n",
    "        \n",
    "        return \"Krotov-Hopfield Hebbian learning rule (delta={:.2f}, lebesgue_p={:.2f}, ranking_param={})\".format(self.delta, self.lebesgue_p, self.ranking_param)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"Return string representation of the object\"\"\"\n",
    "        \n",
    "        return str(self)\n",
    "    \n",
    "    def __call__(self,\n",
    "                 inputs  : torch.Tensor,\n",
    "                 weights : torch.Tensor,\n",
    "                 isconv  : bool = False) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Compute the change of `weights` given by the Krotov learning rule (eq. 3 from [1], with R=1)\n",
    "        for a given batch of `inputs`. See \"A fast implementation\" section in [1].\n",
    "        \n",
    "        The formula is:\n",
    "        ```delta_weights = g(currents) @ inputs - normalization_mtx (*) weights\n",
    "        currents = (sgn(weights) (*) abs(weights) ** lebesgue_p) @ inputs.T```\n",
    "        \n",
    "        where `normalization_mtx` is a matrix of the same shape of `weights`, with all columns equal to:\n",
    "        ```\\sum_{batches} [g(currents) (*) currents]```\n",
    "        \n",
    "        The symbol `@` denotes matrix multiplication, while `(*)` is element-wise multiplication (Hadamard product).\n",
    "        Finally, the function `g` (eq. 10 in [1]) returns:\n",
    "        ```g(currents[i,j]) = 1      if currents[i,j] is the highest in the j-th column (sample),\n",
    "                              -Delta if currets[i,j] is the k-th highest value in the j-th column (sample),\n",
    "                               0     otherwise```\n",
    "        \n",
    "         \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : torch.Tensor of shape (batch_size, input_size)\n",
    "            Batch of inputs\n",
    "        weights : torch.Tensor of shape (output_size, input_size)\n",
    "            Model's weights\n",
    "        isconv : bool\n",
    "            True if the layer is nn.Conv2d, False if it is nn.Linear\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        delta_weights : torch.Tensor of shape (output_size, input_size)\n",
    "            Change of weights given by the fast implementation of Krotov learning rule. The tensor is normalized\n",
    "            so that its maximum is equal to 1.\n",
    "        \"\"\"\n",
    "                \n",
    "        batch_size = inputs.shape[0]\n",
    "        \n",
    "        if not isconv: #Linear logic\n",
    "            #---Currents---#\n",
    "            inputs = torch.t(inputs) #Shape is (batch_size, input_size) -> (input_size, batch_size)\n",
    "            currents = torch.matmul(torch.sign(weights) * torch.abs(weights) ** (self.lebesgue_p - 1), inputs) #Shape is (output_size, batch_size)\n",
    "\n",
    "            #---Activations---#\n",
    "            _, ranking_indices = currents.topk(self.ranking_param, dim=0) #Shape is (self.ranking_param, batch_size)\n",
    "            #Indices of the top k currents produced by each input sample\n",
    "\n",
    "            post_activation_currents = torch.zeros_like(currents) #Shape is (output_size, batch_size)\n",
    "            #Computes g(currents)\n",
    "            #Note that all activations are 0, except the largest current (activation of 1) and the k-th largest (activation of -delta)        \n",
    "\n",
    "            batch_indices = torch.arange(batch_size, device=post_activation_currents.device) \n",
    "            post_activation_currents[ranking_indices[0], batch_indices] = 1.0\n",
    "            post_activation_currents[ranking_indices[self.ranking_param-1], batch_indices] = -self.delta\n",
    "\n",
    "            #---Compute change of weights---#\n",
    "            delta_weights = torch.matmul(post_activation_currents, torch.t(inputs)) #Overlap between post_activation_currents and inputs\n",
    "            second_term   = torch.sum(torch.mul(post_activation_currents, currents), dim=1) #Overlap between currents and post_activation_currents\n",
    "            #Results are summed over batches, resulting in a shape of (output_size,)        \n",
    "\n",
    "            delta_weights = delta_weights - second_term.unsqueeze(1) * weights\n",
    "\n",
    "        else: #Convolutional logic\n",
    "            batch_size = inputs.shape[0]\n",
    "\n",
    "            currents = F.conv2d(input=inputs, weight=(torch.sign(weights) * torch.abs(weights) ** self.lebesgue_p)) \n",
    "            #(batch_size, out_channel, out_height, out_width)\n",
    "\n",
    "\n",
    "            original_shape = currents.shape\n",
    "            _, ranking_indices = currents.view(batch_size, -1).topk(self.ranking_param, dim=1)\n",
    "            ranking_indices = torch.t(ranking_indices)\n",
    "\n",
    "            batch_indices = torch.arange(batch_size, device=inputs.device)\n",
    "\n",
    "            post_activation_currents = torch.zeros((batch_size, np.prod(original_shape[1:])), device=inputs.device, dtype=inputs.dtype)\n",
    "            post_activation_currents[batch_indices, ranking_indices[0]] = 1.0\n",
    "            post_activation_currents[batch_indices, ranking_indices[self.ranking_param -1 ]] = -self.delta\n",
    "            post_activation_currents = post_activation_currents.reshape(original_shape)\n",
    "\n",
    "\n",
    "            delta_weights = F.conv2d(inputs.permute(1, 0, 2, 3), post_activation_currents.permute(1, 0, 2, 3)).permute(1, 0, 2, 3)\n",
    "\n",
    "            term = torch.sum(post_activation_currents * currents, dim = 0)\n",
    "            \n",
    "            ones = torch.ones(inputs.shape[1:], device=inputs.device, dtype=inputs.dtype) # / np.prod(inputs.shape[2:])\n",
    "\n",
    "            term2 = F.conv2d(ones.unsqueeze(1), term.unsqueeze(1)).permute(1, 0, 2, 3)\n",
    "\n",
    "            delta_weights = delta_weights - term2 * weights\n",
    "            \n",
    "        #---Normalize---#\n",
    "        nc = torch.max(torch.abs(delta_weights)) \n",
    "        if nc < self.precision:\n",
    "            nc = self.precision\n",
    "\n",
    "        return delta_weights / nc #Maximum (absolute) change of weight is set to +1. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "typical-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 output_dim : int = 10,\n",
    "                 n : int = 1,\n",
    "                 beta : float = .1):\n",
    "        \"\"\"Builds the neural network architecture presented in [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_dim : int\n",
    "            Output dimensionality (number of classes)\n",
    "        n : int\n",
    "            Exponent of activation function for the first hidden layer (n = 1 for ReLU). See eq. 1 in [1].\n",
    "        beta : float\n",
    "            Parameter for the activation function in the top layer (akin to an \"inverse temperature\"). See eq. 1 in [1].         \n",
    "        \n",
    "        [1]: \"Unsupervised learning by competing hidden units\", D. Krotov, J. Hopfield, 2019\n",
    "        \"\"\"\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        #Store parameters\n",
    "        self.n = n\n",
    "        self.beta = beta\n",
    "        \n",
    "        #Define layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=10, bias=False)\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 5, kernel_size=8, bias=False)\n",
    "        self.fc1 = nn.Linear(2,3)\n",
    "        \n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the output of the network for the input `x`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor of shape (batch_size, self.input_dim)\n",
    "            Batch of input values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : torch.Tensor of shape (batch_size, self.output_dim)\n",
    "            Batch of output values\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = max_pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quantitative-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsupervisedPhase(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Perform unsupervised learning of the first layers of a sequential network according to some local learning rule.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model               : torch.nn.Sequential,\n",
    "                 update_weights_rule : Callable,\n",
    "                 supervised_from     : int = -1,\n",
    "                 freeze_layers       : List[str] = None,\n",
    "                 n_epochs            : int = 100\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : torch.nn.Sequential\n",
    "            Model to be trained. Must be a sequence of Linear layers (+ some activations).\n",
    "        update_weights_rule : function\n",
    "            Function that computes the change of weights\n",
    "        supervised_from : int\n",
    "            Train all layers that appear *before* `supervised_from`. By default, all layers except the very last one are trained.\n",
    "        freeze_layers : list of strings\n",
    "            Names of layers to be excluded from training.\n",
    "        n_epochs : int\n",
    "            Number of epochs for the full training (needed to set the learning rate)\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #---Store arguments---#\n",
    "        self.model           = model\n",
    "        self.supervised_from = supervised_from\n",
    "        self.freeze_layers   = freeze_layers\n",
    "        self.update_weights_rule = update_weights_rule\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        if self.freeze_layers is None:\n",
    "            self.freeze_layers = []\n",
    "            \n",
    "        self.layers = [] #Save all layers that need to be trained\n",
    "        Layer = namedtuple('Layer', ['idx', 'name', 'layer']) #simple object to store layer metadata\n",
    "        \n",
    "        for idx, (name, layer) in enumerate(list(model.named_children())[:self.supervised_from]):\n",
    "            if (type(layer) == torch.nn.Linear or type(layer) == torch.nn.Conv2d) and name not in self.freeze_layers:\n",
    "                self.layers.append(Layer(idx, name, layer))\n",
    "                \n",
    "                layer.weight.data.normal_(mean=0.0, std=1.0) #and initialize weights with normal distribution\n",
    "        \n",
    "        print(\"Layers selected for unsupervised local (bio)learning:\")\n",
    "        print(\"{} layer(s): \".format(len(self.layers)), [lyr.name for lyr in self.layers])\n",
    "        \n",
    "        #Register hooks in trainable layers so that input/output can be stored (which is needed for Hebbian learning)\n",
    "        self._hooks = {} \n",
    "        self._inputs = {}\n",
    "        self._outputs = {}\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            self._hooks[layer.name] = layer.layer.register_forward_hook(\n",
    "                partial(self._store_data_hook, layer_name=layer.name)\n",
    "            )\n",
    "            \n",
    "        self.automatic_optimization = False #Disable automatic gradient optimizations \n",
    "        #(since we are using a local optimizer, which does not need any gradient)\n",
    "        \n",
    "    def forward(self, x : torch.Tensor):\n",
    "        \"\"\"Compute model output\"\"\"\n",
    "        \n",
    "        return self.model.forward(x)\n",
    "    \n",
    "    def _store_data_hook(self, _, inp, out, layer_name):\n",
    "        \"\"\"Hook for storing input/outputs of trainable layers\"\"\"\n",
    "\n",
    "        self._inputs[layer_name] = inp[0]\n",
    "        self._outputs[layer_name] = out\n",
    "    \n",
    "        \n",
    "    def training_step(self, batch : torch.Tensor, batch_idx : int):\n",
    "        \"\"\"\n",
    "        Single iteration of the training cycle.\n",
    "        \"\"\"\n",
    "        \n",
    "        opt = self.optimizers() #Retrieve the optimizer\n",
    "        \n",
    "        with torch.no_grad(): #No gradient is needed\n",
    "            x, y = batch\n",
    "\n",
    "            #---Forward step---#\n",
    "            layers = list(self.model.children())\n",
    "            for layer in layers[:self.supervised_from]:  #-1\n",
    "                x = layer(x)\n",
    "            \n",
    "            #---Unsupervised learning---#\n",
    "            for layer_idx, layer_name, layer in self.layers:\n",
    "                inputs = self._inputs[layer_name]\n",
    "                    \n",
    "                weights = layer.weight\n",
    "                \n",
    "                isconv = (type(layer) == torch.nn.Conv2d)\n",
    "                \n",
    "                #Compute change of weights\n",
    "                delta_weights = self.update_weights_rule(inputs, weights, isconv)\n",
    "                delta_weights = delta_weights.view(*layer.weight.size()) #Flatten\n",
    "                \n",
    "                #Update weights\n",
    "                opt.local_step(delta_weights, layer_name=layer_name)\n",
    "                \n",
    "        return x, y\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Define optimizer and learning rate schedule.\"\"\"\n",
    "        \n",
    "        start_lr = .015\n",
    "        optimizer = Local(named_params = self.model.named_parameters(), lr=start_lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch : (self.n_epochs - epoch) / self.n_epochs,\n",
    "                                                         verbose=True)\n",
    "        \n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "split-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_weights(t, fig=None):\n",
    "    \n",
    "    #kernels depth * number of kernels\n",
    "    nplots = t.shape[0]*t.shape[1]\n",
    "    ncols = 6\n",
    "    \n",
    "    nrows = 1 + nplots//ncols\n",
    "    #convert tensor to numpy image\n",
    "    npimg = np.array(t.numpy(), np.float32)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    if fig is None:\n",
    "        fig = plt.figure(figsize=(ncols, nrows))\n",
    "    \n",
    "    #looping through all the kernels in each channel\n",
    "    for i in range(t.shape[0]):\n",
    "        for j in range(t.shape[1]):\n",
    "            count += 1\n",
    "            ax1 = fig.add_subplot(nrows, ncols, count)\n",
    "            npimg = np.array(t[i, j].numpy(), np.float32)\n",
    "            npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
    "            npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
    "            ax1.imshow(npimg, cmap='bwr', interpolation=None)\n",
    "            ax1.set_title(str(i) + ',' + str(j))\n",
    "            ax1.axis('off')\n",
    "            ax1.set_xticklabels([])\n",
    "            ax1.set_yticklabels([])\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "irish-regular",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(gpus=[0], max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "popular-devices",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krotov-Hopfield Hebbian learning rule (delta=0.40, lebesgue_p=3.00, ranking_param=2)\n",
      "CNN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(10, 10), stride=(1, 1), bias=False)\n",
      "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 5, kernel_size=(8, 8), stride=(1, 1), bias=False)\n",
      "  (fc1): Linear(in_features=2, out_features=3, bias=True)\n",
      ")\n",
      "Layers selected for unsupervised local (bio)learning:\n",
      "2 layer(s):  ['conv1', 'conv2']\n"
     ]
    }
   ],
   "source": [
    "mnist = MNISTDataModule(train_batch_size=16)\n",
    "update_weights_rule = BioLearningRule(delta=.4, lebesgue_p=2, ranking_param=2)\n",
    "net = CNN(10).double()\n",
    "print(net)\n",
    "\n",
    "full_system = UnsupervisedPhase(model=net, update_weights_rule=update_weights_rule, n_epochs=trainer.max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "experienced-calculation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\Anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:50: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.5000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | CNN  | 6.7 K \n",
      "-------------------------------\n",
      "6.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e11cc0be3c04613bb9cfb25d6c5f543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Adjusting learning rate of group 0 to 5.0000e-03.\n",
      "Adjusting learning rate of group 0 to 0.0000e+00.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(full_system, mnist) #Run training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "collective-butter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 380.975x190.488 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with mpl2latex(True):\n",
    "    fig = plt.figure(figsize=latex_figsize(wf=1.2, hf=.5, columnwidth=318.67))\n",
    "    fig.patch.set_facecolor('none')\n",
    "    \n",
    "    weights = next(full_system.layers[0].layer.parameters()).detach().cpu()\n",
    "    plot_weights(weights[:, :, :, :], fig=fig)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig(\"Plots/CNN_weights.pdf\", transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "polish-webmaster",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 380.975x304.78 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with mpl2latex(True):\n",
    "    fig = plt.figure(figsize=latex_figsize(wf=1.2, hf=.8, columnwidth=318.67))\n",
    "    fig.patch.set_facecolor('none')\n",
    "    \n",
    "    weights = next(full_system.layers[1].layer.parameters()).detach().cpu()\n",
    "    plot_weights(weights[:5, :5, :, :], fig=fig)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig(\"Plots/CNN_weights_2.pdf\", transparent=True, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "eea8b80adef1497d7dfb4c0448d9c7aaf24335e0b82315474639507e757c963e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
